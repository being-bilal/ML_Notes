{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1) # fetching the mnist dataset\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.DESCR)  # printing the description of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = mnist['data'], mnist['target'] \n",
    "x = x.to_numpy()\n",
    "y = y.to_numpy() # turning pandas Dataframe to Numpy array\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "some_digit = x[2] \n",
    "y = y.astype(np.int32)  # converting the labels to integers\n",
    "some_digit_image = some_digit.reshape(28, 28) \n",
    "plt.imshow(some_digit_image, cmap=\"binary\")\n",
    "plt.show() # displaying random mnist digit\n",
    "print(y[2])  # printing the label of the digit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting training and testing data\n",
    "x_train, x_test, y_train, y_test = x[:60000], x[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stochastic Gradient Descent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarry classsifier for digit 5\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "y_train_5 = (y_train == 5)  # creating a binary target variable for digit 5\n",
    "y_test_5 = (y_test == 5)\n",
    "# schochastic gradient descent classifier\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(x_train, y_train_5)  # training the classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting if the digit is 5 or not\n",
    "test_digit = x[0]\n",
    "sgd_clf.predict([test_digit]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring Accuracy\n",
    "# accuracy is not a good measure for imbalanced datasets as the classifier can achieve high accuracy by simply predicting the most likely class\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# cross_val_score is used to evaluate the performance of the model using cross-validation\n",
    "cross_val_score(sgd_clf, x_train, y_train_5, cv = 3)  # performing cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "# cross_val_predict is used to get the predictions for each instance in the training set using cross-validation\n",
    "y_train_pred = cross_val_predict(sgd_clf, x_train, y_train_5, cv=3) \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "# creating a confusion matrix\n",
    "cm = confusion_matrix(y_train_5, y_train_pred)  \n",
    "print(cm)\n",
    "\n",
    "# calculating precision, recall andn F1 score\n",
    "precision = precision_score(y_train_5, y_train_pred)\n",
    "recall = recall_score(y_train_5, y_train_pred)\n",
    "f1 = f1_score(y_train_5, y_train_pred) # F1 score is the most reliable metric \n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1) \n",
    "# Precision/Recall Trade-off : precision and recall are inversely proportional to each other, model with high pprecision will have low recall and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the decision function score\n",
    "# descision function score tells how cofident the model is aboout its prediction \n",
    "# threshold is the point above which the model will classify a value as true and below which it will classify it as false\n",
    "test_digit = x[0]\n",
    "y_score = sgd_clf.decision_function([test_digit])  # getting the decision function score for the test digit\n",
    "print(y_score)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the decision function scoore is greater thaan the threshold, the model would return true otherwise it would return false \n",
    "threshold = 0 # threshold is set to 0 by default\n",
    "y_pred = (y_score > threshold)\n",
    "print(y_pred)  \n",
    "\n",
    "# Changing the threshold \n",
    "# increasing the threshold will increase the prescision and decrease the recall\n",
    "threshold = 5000\n",
    "y_pred = (y_score > threshold) \n",
    "print(y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the best threshold \n",
    "# the threshold to achieve a particular precison or recall can be achieved using the precision-recall curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# getting the decision function scores for the training set using cross-validation\n",
    "y_scores = cross_val_predict(sgd_clf, x_train, y_train_5, cv=3, method=\"decision_function\")\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "\n",
    "# plotting the precision-recall curve\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylim([0, 1])\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()\n",
    "# the precision curve is more bumpier than the recall curve, this is because the precision is more sensitive to the threshold than the recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the optimal threshold \n",
    "print(len(precisions), len(recalls), len(thresholds)) # the length of the precisions, recalls is 1 greater than the length of the thresholds\n",
    "f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1])\n",
    "best_f1_value = np.argmax(f1_scores)  # getting the index of the best F1 score\n",
    "optimal_threshold = thresholds[best_f1_value]  \n",
    "print(\"Best F1 Score:\", f1_scores[best_f1_value])\n",
    "print(f\"optimal threshold : {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold for 90% precision \n",
    "best_precision = np.argmax(precisions >= 0.9)  # getting the index of the first precision value greater than or equal to 0.9 \n",
    "new_threshold = thresholds[best_precision]\n",
    "print(f\"threshold for 90% precision: {new_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve \n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "# ROC curve is a curve between the true positive rate (TPR) and the false positive rate (FPR)\n",
    "fpr, tpr, thresholdv = roc_curve(y_train_5, y_scores)\n",
    "# Plotting the ROC curve \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve)')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# AUC (Area Under the Curve) is a single number that summarizes the performance of the classifier\n",
    "auc = roc_auc_score(y_train_5, y_scores)\n",
    "print(f\"AUC: {auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# MultiClass Classification \n",
    "from sklearn.svm import SVC\n",
    "from random import randint\n",
    "\n",
    "svm_clf = SVC()\n",
    "random_num = randint(0, 59999)\n",
    "test_digit = x[random_num] # random digit from the training set\n",
    "# SVC is a binary classifier but it can be used for multiclass classification by using the one-vs-one or one-vs-all strategy\n",
    "svm_clf.fit(x_train, y_train) # SVC uses the one-vs-one strategy by default\n",
    "print(svm_clf.predict([test_digit]))  \n",
    "print(y[random_num])  # printing the label of the digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.76247599  9.30488756  5.84432361  7.24918342  0.73788086 -0.29284998\n",
      "   3.78547267  1.72988609  8.30228948  2.74859509]]\n"
     ]
    }
   ],
   "source": [
    "# printing the decision function score for the test digit\n",
    "y_scores = svm_clf.decision_function([test_digit])\n",
    "print(y_scores) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
